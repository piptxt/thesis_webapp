{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66dec151-86a0-43a1-8f96-1e79033f8ab1",
   "metadata": {},
   "source": [
    "## Legal Summarization Dataset [Metric: Normalized Discounted Cumulative Gain @ 10 (nDCG@10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c814efe-15e1-458a-b7a1-44c4b1b7323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Download the dataset\n",
    "queries_dataset = datasets.load_dataset(\"mteb/legal_summarization\", \"queries\")\n",
    "documents_dataset = datasets.load_dataset(\"mteb/legal_summarization\", \"corpus\")\n",
    "pair_labels_dataset = datasets.load_dataset(\"mteb/legal_summarization\", \"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32146317-a5f9-413d-83d2-6c8c4feb3fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    queries: Dataset({\n",
      "        features: ['_id', 'text'],\n",
      "        num_rows: 284\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    corpus: Dataset({\n",
      "        features: ['_id', 'title', 'text'],\n",
      "        num_rows: 438\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['query-id', 'corpus-id', 'score'],\n",
      "        num_rows: 439\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(queries_dataset)\n",
    "print(documents_dataset)\n",
    "print(pair_labels_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "757a1e86-01a0-4f1f-bd95-b82798393223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract queries and documents using appropriate keys\n",
    "queries = [item['text'] for item in queries_dataset['queries']]\n",
    "documents = [item['text'] for item in documents_dataset['corpus']]\n",
    "pair_labels = [(pair['query-id'], pair['corpus-id'], pair['score']) for pair in pair_labels_dataset['test']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f1196b-da45-4acf-a7bc-845e96d1a7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3Updated\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer('Alibaba-NLP/gte-large-en-v1.5', trust_remote_code=True)\n",
    "\n",
    "# Function to embed a single text\n",
    "def embed_text(text):\n",
    "    return model.encode(text, convert_to_tensor=True, device='cuda')\n",
    "\n",
    "# Embed the queries and documents one by one\n",
    "query_embeddings = torch.stack([embed_text(query) for query in queries])\n",
    "document_embeddings = torch.stack([embed_text(doc) for doc in documents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2586b95c-3ee7-417f-b8d6-9959d7ba31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# Compute cosine similarities\n",
    "cosine_similarities = cos_sim(query_embeddings, document_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50b137f1-d6f6-485f-970a-9a0930f81b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average nDCG@10: 0.4527647304337138\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# Compute cosine similarities\n",
    "cosine_similarities = cos_sim(query_embeddings, document_embeddings)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Prepare a mapping from IDs to indices\n",
    "query_id_to_index = {id: index for index, id in enumerate(queries_dataset['queries']['_id'])}\n",
    "corpus_id_to_index = {id: index for index, id in enumerate(documents_dataset['corpus']['_id'])}\n",
    "\n",
    "def calculate_ndcg(cosine_similarities, pair_labels, k=10):\n",
    "    ndcg_scores = []\n",
    "    for query_id, corpus_id, score in pair_labels:\n",
    "        query_index = query_id_to_index[query_id]\n",
    "        corpus_index = corpus_id_to_index[corpus_id]\n",
    "        \n",
    "        # Create true relevance scores based on the scores in the dataset\n",
    "        true_relevance = np.zeros(len(documents))\n",
    "        true_relevance[corpus_index] = score\n",
    "\n",
    "        # Get similarities for the current query\n",
    "        query_similarities = cosine_similarities[query_index].cpu().numpy()\n",
    "\n",
    "        # Calculate nDCG@10\n",
    "        ndcg = ndcg_score([true_relevance], [query_similarities], k=k)\n",
    "        ndcg_scores.append(ndcg)\n",
    "    \n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "# Calculate and print the average nDCG@10\n",
    "average_ndcg_at_10 = calculate_ndcg(cosine_similarities, pair_labels)\n",
    "print(f\"Average nDCG@10: {average_ndcg_at_10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335dc9a8-bf8f-470c-96e8-55923cb7fe06",
   "metadata": {},
   "source": [
    "## ConsumerContractsQA [Metric: Normalized Discounted Cumulative Gain @ 10 (nDCG@10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb2b5b1-bff6-4ea3-8c34-5e5d67a0fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Download the dataset\n",
    "queries_dataset = datasets.load_dataset(\"mteb/legalbench_consumer_contracts_qa\", \"queries\")\n",
    "documents_dataset = datasets.load_dataset(\"mteb/legalbench_consumer_contracts_qa\", \"corpus\")\n",
    "pair_labels_dataset = datasets.load_dataset(\"mteb/legalbench_consumer_contracts_qa\", \"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af84c936-3bb5-4811-9cd5-96b2812a70ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    queries: Dataset({\n",
      "        features: ['_id', 'text'],\n",
      "        num_rows: 396\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    corpus: Dataset({\n",
      "        features: ['_id', 'title', 'text'],\n",
      "        num_rows: 154\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['query-id', 'corpus-id', 'score'],\n",
      "        num_rows: 396\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Inspect datasets to determine the correct keys\n",
    "print(queries_dataset)\n",
    "print(documents_dataset)\n",
    "print(pair_labels_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dff786b-09d7-423a-9ec4-105ba3741e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract queries and documents using appropriate keys\n",
    "queries = [item['text'] for item in queries_dataset['queries']]\n",
    "documents = [item['text'] for item in documents_dataset['corpus']]\n",
    "pair_labels = [(pair['query-id'], pair['corpus-id'], pair['score']) for pair in pair_labels_dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "185ec216-3e3c-4ce3-9e0a-d3f4b1683aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3Updated\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer('Alibaba-NLP/gte-large-en-v1.5', trust_remote_code=True)\n",
    "\n",
    "# Function to embed a single text\n",
    "def embed_text(text):\n",
    "    return model.encode(text, convert_to_tensor=True, device='cuda')\n",
    "\n",
    "# Embed the queries and documents one by one\n",
    "query_embeddings = torch.stack([embed_text(query) for query in queries])\n",
    "document_embeddings = torch.stack([embed_text(doc) for doc in documents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b86831d-083a-44fc-9a36-de7668d8d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# Compute cosine similarities\n",
    "cosine_similarities = cos_sim(query_embeddings, document_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae941019-efc1-49f4-9178-484c231cdb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average nDCG@10: 0.8011939407382996\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# Compute cosine similarities\n",
    "cosine_similarities = cos_sim(query_embeddings, document_embeddings)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Prepare a mapping from IDs to indices\n",
    "query_id_to_index = {id: index for index, id in enumerate(queries_dataset['queries']['_id'])}\n",
    "corpus_id_to_index = {id: index for index, id in enumerate(documents_dataset['corpus']['_id'])}\n",
    "\n",
    "def calculate_ndcg(cosine_similarities, pair_labels, k=10):\n",
    "    ndcg_scores = []\n",
    "    for query_id, corpus_id, score in pair_labels:\n",
    "        query_index = query_id_to_index[query_id]\n",
    "        corpus_index = corpus_id_to_index[corpus_id]\n",
    "        \n",
    "        # Create true relevance scores based on the scores in the dataset\n",
    "        true_relevance = np.zeros(len(documents))\n",
    "        true_relevance[corpus_index] = score\n",
    "\n",
    "        # Get similarities for the current query\n",
    "        query_similarities = cosine_similarities[query_index].cpu().numpy()\n",
    "\n",
    "        # Calculate nDCG@10\n",
    "        ndcg = ndcg_score([true_relevance], [query_similarities], k=k)\n",
    "        ndcg_scores.append(ndcg)\n",
    "    \n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "# Calculate and print the average nDCG@10\n",
    "average_ndcg_at_10 = calculate_ndcg(cosine_similarities, pair_labels)\n",
    "print(f\"Average nDCG@10: {average_ndcg_at_10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd0992c-c6f2-4417-a0f8-4643263f6b25",
   "metadata": {},
   "source": [
    "## CorporateLobbying [Metric: Normalized Discounted Cumulative Gain @ 10 (nDCG@10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44b10898-53ff-46bc-8039-40ca982baf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Download the dataset\n",
    "queries_dataset = datasets.load_dataset(\"mteb/legalbench_corporate_lobbying\", \"queries\")\n",
    "documents_dataset = datasets.load_dataset(\"mteb/legalbench_corporate_lobbying\", \"corpus\")\n",
    "pair_labels_dataset = datasets.load_dataset(\"mteb/legalbench_corporate_lobbying\", \"default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a9a0bdd-4327-4d78-bb9a-f8a228feb94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    queries: Dataset({\n",
      "        features: ['_id', 'text'],\n",
      "        num_rows: 340\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    corpus: Dataset({\n",
      "        features: ['_id', 'title', 'text'],\n",
      "        num_rows: 319\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['query-id', 'corpus-id', 'score'],\n",
      "        num_rows: 340\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(queries_dataset)\n",
    "print(documents_dataset)\n",
    "print(pair_labels_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "714e559d-af7b-421b-82ae-ec9e2492ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract queries and documents using appropriate keys\n",
    "queries = [item['text'] for item in queries_dataset['queries']]\n",
    "documents = [item['text'] for item in documents_dataset['corpus']]\n",
    "pair_labels = [(pair['query-id'], pair['corpus-id'], pair['score']) for pair in pair_labels_dataset['test']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29249ed9-14cb-45e7-bbc1-3d7c379dd77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3Updated\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer('Alibaba-NLP/gte-large-en-v1.5', trust_remote_code=True)\n",
    "\n",
    "# Function to embed a single text\n",
    "def embed_text(text):\n",
    "    return model.encode(text, convert_to_tensor=True, device='cuda')\n",
    "\n",
    "# Embed the queries and documents one by one\n",
    "query_embeddings = torch.stack([embed_text(query) for query in queries])\n",
    "document_embeddings = torch.stack([embed_text(doc) for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a9f348a-a2c2-4136-b4e8-513856ef76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# Compute cosine similarities\n",
    "cosine_similarities = cos_sim(query_embeddings, document_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ad9a8ae-c09b-42bb-a415-fe2df6016412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average nDCG@10: 0.8849441186346295\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# Compute cosine similarities\n",
    "cosine_similarities = cos_sim(query_embeddings, document_embeddings)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Prepare a mapping from IDs to indices\n",
    "query_id_to_index = {id: index for index, id in enumerate(queries_dataset['queries']['_id'])}\n",
    "corpus_id_to_index = {id: index for index, id in enumerate(documents_dataset['corpus']['_id'])}\n",
    "\n",
    "def calculate_ndcg(cosine_similarities, pair_labels, k=10):\n",
    "    ndcg_scores = []\n",
    "    for query_id, corpus_id, score in pair_labels:\n",
    "        query_index = query_id_to_index[query_id]\n",
    "        corpus_index = corpus_id_to_index[corpus_id]\n",
    "        \n",
    "        # Create true relevance scores based on the scores in the dataset\n",
    "        true_relevance = np.zeros(len(documents))\n",
    "        true_relevance[corpus_index] = score\n",
    "\n",
    "        # Get similarities for the current query\n",
    "        query_similarities = cosine_similarities[query_index].cpu().numpy()\n",
    "\n",
    "        # Calculate nDCG@10\n",
    "        ndcg = ndcg_score([true_relevance], [query_similarities], k=k)\n",
    "        ndcg_scores.append(ndcg)\n",
    "    \n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "# Calculate and print the average nDCG@10\n",
    "average_ndcg_at_10 = calculate_ndcg(cosine_similarities, pair_labels)\n",
    "print(f\"Average nDCG@10: {average_ndcg_at_10}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
